[package]
name = "rust_ollama"
version = "0.2.0"
edition = "2021"
authors = ["MiniMax Agent"]
description = "A high-performance, modular LLM inference server with Ollama-compatible API"
license = "MIT"

[dependencies]
# Web framework
axum = { version = "0.7", features = ["json", "headers", "ws", "multipart"] }
tower = { version = "0.4", features = ["util", "timeout", "limit", "load-shed"] }
tower-http = { version = "0.5", features = ["cors", "trace", "compression-br"] }
hyper = { version = "1.0", features = ["full"] }
tokio = { version = "1.0", features = ["full"] }
warp = "0.3"

# HTTP client/server utilities
reqwest = { version = "0.11", features = ["json", "stream"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bytes = "1.0"
http = "1.0"

# LLM Inference (Enhanced)
candle-core = "0.6"
candle-nn = "0.6"
candle-transformers = "0.6"
candle-text-embeddings = "0.6"
tokenizers = "0.19"

# Database
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono", "json"] }
tokio-rsqlite = "0.5"

# Real-time communication
tungstenite = { version = "0.21", features = ["native-tls"] }
websocket = "0.3"

# Performance monitoring
prometheus = "0.13"
metrics = "0.22"
metrics-exporter-prometheus = "0.12"
opentelemetry = { version = "0.18", features = ["metrics"] }
opentelemetry-otlp = "0.11"
tracing-opentelemetry = "0.18"

# Utilities
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["fmt", "chrono", "env-filter"] }
clap = { version = "4.0", features = ["derive", "cargo"] }
config = "0.14"
once_cell = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
tokio-util = { version = "0.7", features = ["codec"] }
futures = "0.3"
async-trait = "0.1"
dashmap = "5.4"
parking_lot = "0.12"

# Logging
env_logger = "0.10"
log = "0.4"

# CLI enhancements
crossterm = { version = "0.27", features = ["event-stream"] }
tui = { version = "0.19", features = ["crossterm"] }
serde_yaml = "0.9"

# Vector embeddings and search
annoy = "0.2"

# Machine learning utilities
ndarray = "0.15"

[build-dependencies]
cc = "1.0"

[features]
default = ["candle-cuda", "websocket-support", "advanced-monitoring"]
candle-cuda = ["candle-core/cuda", "candle-nn/cuda"]
metal = ["candle-core/metal"]
websocket-support = ["tungstenite"]
advanced-monitoring = ["opentelemetry", "opentelemetry-otlp"]
vector-search = ["annoy"]

[[bin]]
name = "rust_ollama"
path = "src/main.rs"

[[bin]]
name = "ollama_cli"
path = "src/bin/ollama_cli.rs"

[[bin]]
name = "ollama_tui"
path = "src/bin/ollama_tui.rs"

[[bin]]
name = "model_finetuner"
path = "src/bin/model_finetuner.rs"

[[bin]]
name = "stress_test"
path = "src/bin/stress_test.rs"