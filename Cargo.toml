[package]
name = "rust_ollama"
version = "0.1.0"
edition = "2021"
authors = ["MiniMax Agent"]
description = "A modular Rust-based LLM inference server with Ollama-compatible API"
license = "MIT"

[dependencies]
# Web framework
axum = { version = "0.7", features = ["json", "headers", "ws", "multipart"] }
tower = { version = "0.4", features = ["util", "timeout", "limit", "load-shed"] }
hyper = { version = "1.0", features = ["full"] }
tokio = { version = "1.0", features = ["full"] }
warp = "0.3"

# HTTP client/server utilities
reqwest = { version = "0.11", features = ["json", "stream"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
bytes = "1.0"
http = "1.0"

# LLM Inference
candle-core = "0.6"
candle-nn = "0.6"
candle-transformers = "0.6"
candle-text-embeddings = "0.6"
tokenizers = "0.19"

# Database
sqlx = { version = "0.7", features = ["runtime-tokio-rustls", "sqlite", "chrono", "json"] }
tokio-rsqlite = "0.5"

# Utilities
anyhow = "1.0"
thiserror = "1.0"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["fmt", "chrono"] }
clap = { version = "4.0", features = ["derive", "cargo"] }
config = "0.14"
once_cell = "1.0"
uuid = { version = "1.0", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
tokio-util = { version = "0.7", features = ["codec"] }
futures = "0.3"
async-trait = "0.1"

# Logging
env_logger = "0.10"
log = "0.4"

[build-dependencies]
cc = "1.0"

[features]
default = ["candle-cuda"]
candle-cuda = ["candle-core/cuda", "candle-nn/cuda"]
metal = ["candle-core/metal"]

[[bin]]
name = "rust_ollama"
path = "src/main.rs"

[[bin]]
name = "ollama_cli"
path = "src/bin/ollama_cli.rs"